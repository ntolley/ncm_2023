{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n",
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import pickle\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import contrastive_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = contrastive_functions.get_marker_decode_dataframes()\n",
    "wrist_df = data_dict['wrist_df']\n",
    "task_neural_df = data_dict['task_neural_df']\n",
    "notask_neural_df = data_dict['notask_neural_df']\n",
    "metadata = data_dict['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = task_neural_df['trial'].unique()\n",
    "num_trials_filtered = len(trial_ids)\n",
    "\n",
    "#Generate cv_dict for regular train/test/validate split\n",
    "cv_split = ShuffleSplit(n_splits=5, test_size=.25, random_state=3)\n",
    "val_split = ShuffleSplit(n_splits=1, test_size=.25, random_state=3)\n",
    "cv_dict = {}\n",
    "for fold, (train_val_idx, test_idx) in enumerate(cv_split.split(trial_ids)):\n",
    "    for t_idx, v_idx in val_split.split(train_val_idx): #No looping, just used to split train/validation sets\n",
    "        cv_dict[fold] = {'train_idx':trial_ids[train_val_idx[t_idx]], \n",
    "                        'test_idx':trial_ids[test_idx], \n",
    "                        'validation_idx':trial_ids[train_val_idx[v_idx]]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.9414  ... Validation Loss: 1.8833\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.5298  ... Validation Loss: 1.4982\n",
      "**.*******\n",
      "Epoch: 30/1000 ... Train Loss: 1.3921  ... Validation Loss: 1.3437\n",
      "******.*.*\n",
      "Epoch: 40/1000 ... Train Loss: 1.3417  ... Validation Loss: 1.2895\n",
      ".**.***.*.\n",
      "Epoch: 50/1000 ... Train Loss: 1.3086  ... Validation Loss: 1.2700\n",
      "**.*.....*\n",
      "Epoch: 60/1000 ... Train Loss: 1.2805  ... Validation Loss: 1.2527\n",
      "*.*..*....\n",
      "Epoch: 70/1000 ... Train Loss: 1.2597  ... Validation Loss: 1.2475\n",
      ".*..*....*\n",
      "Epoch: 80/1000 ... Train Loss: 1.2417  ... Validation Loss: 1.2352\n",
      "...... Early Stop; Min Epoch: 80\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.8768  ... Validation Loss: 0.8269\n",
      "********.*\n",
      "Epoch: 20/1000 ... Train Loss: 0.7003  ... Validation Loss: 0.6854\n",
      "**********\n",
      "Epoch: 30/1000 ... Train Loss: 0.4078  ... Validation Loss: 0.3490\n",
      "**.**.***.\n",
      "Epoch: 40/1000 ... Train Loss: 0.2899  ... Validation Loss: 0.2801\n",
      "*..*.*.**.\n",
      "Epoch: 50/1000 ... Train Loss: 0.2615  ... Validation Loss: 0.2679\n",
      "..*...*...\n",
      "Epoch: 60/1000 ... Train Loss: 0.2458  ... Validation Loss: 0.2520\n",
      "*.*.*.**.*\n",
      "Epoch: 70/1000 ... Train Loss: 0.2244  ... Validation Loss: 0.2380\n",
      "*.****.*.*\n",
      "Epoch: 80/1000 ... Train Loss: 0.2129  ... Validation Loss: 0.2242\n",
      ".*.*.**...\n",
      "Epoch: 90/1000 ... Train Loss: 0.1946  ... Validation Loss: 0.2199\n",
      "... Early Stop; Min Epoch: 87\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.6803  ... Validation Loss: 1.6208\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.2648  ... Validation Loss: 1.3176\n",
      "******.*..\n",
      "Epoch: 30/1000 ... Train Loss: 1.1721  ... Validation Loss: 1.2406\n",
      "*.....*.*.\n",
      "Epoch: 40/1000 ... Train Loss: 1.1491  ... Validation Loss: 1.2300\n",
      "..*..**.*.\n",
      "Epoch: 50/1000 ... Train Loss: 1.1281  ... Validation Loss: 1.2268\n",
      ".*.*......\n",
      "Epoch: 60/1000 ... Train Loss: 1.1176  ... Validation Loss: 1.2154\n",
      " Early Stop; Min Epoch: 54\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.4161  ... Validation Loss: 0.3355\n",
      "*****.**.*\n",
      "Epoch: 20/1000 ... Train Loss: 0.2632  ... Validation Loss: 0.2717\n",
      "..**..*.*.\n",
      "Epoch: 30/1000 ... Train Loss: 0.2455  ... Validation Loss: 0.2656\n",
      "..********\n",
      "Epoch: 40/1000 ... Train Loss: 0.1606  ... Validation Loss: 0.1976\n",
      "*.**.**...\n",
      "Epoch: 50/1000 ... Train Loss: 0.1239  ... Validation Loss: 0.1850\n",
      ".*..*.*...\n",
      "Epoch: 60/1000 ... Train Loss: 0.1092  ... Validation Loss: 0.1768\n",
      "..*....*..\n",
      "Epoch: 70/1000 ... Train Loss: 0.0981  ... Validation Loss: 0.1743\n",
      "..*...... Early Stop; Min Epoch: 73\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.7099  ... Validation Loss: 1.6402\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.2721  ... Validation Loss: 1.3024\n",
      "*******.**\n",
      "Epoch: 30/1000 ... Train Loss: 1.1385  ... Validation Loss: 1.2025\n",
      "**...... Early Stop; Min Epoch: 32\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.4396  ... Validation Loss: 0.3567\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 0.2306  ... Validation Loss: 0.2429\n",
      "**.*****.*\n",
      "Epoch: 30/1000 ... Train Loss: 0.1425  ... Validation Loss: 0.1877\n",
      "*...... Early Stop; Min Epoch: 31\n",
      "****.***.*\n",
      "Epoch: 10/1000 ... Train Loss: 1.9762  ... Validation Loss: 1.9103\n",
      "*********.\n",
      "Epoch: 20/1000 ... Train Loss: 1.8548  ... Validation Loss: 1.8551\n",
      "...**.*.*.\n",
      "Epoch: 30/1000 ... Train Loss: 1.5576  ... Validation Loss: 1.6765\n",
      "*****.**..\n",
      "Epoch: 40/1000 ... Train Loss: 1.4899  ... Validation Loss: 1.6124\n",
      "*.**.**.**\n",
      "Epoch: 50/1000 ... Train Loss: 1.4739  ... Validation Loss: 1.5383\n",
      ".***..*.**\n",
      "Epoch: 60/1000 ... Train Loss: 1.4151  ... Validation Loss: 1.4541\n",
      "....*...**\n",
      "Epoch: 70/1000 ... Train Loss: 1.4000  ... Validation Loss: 1.4367\n",
      ".*.**.....\n",
      "Epoch: 80/1000 ... Train Loss: 1.3812  ... Validation Loss: 1.4309\n",
      ". Early Stop; Min Epoch: 75\n",
      "****.*.***\n",
      "Epoch: 10/1000 ... Train Loss: 0.9486  ... Validation Loss: 0.8832\n",
      ".*.*******\n",
      "Epoch: 20/1000 ... Train Loss: 0.8805  ... Validation Loss: 0.8408\n",
      "**.****.*.\n",
      "Epoch: 30/1000 ... Train Loss: 0.5532  ... Validation Loss: 0.6420\n",
      "*...... Early Stop; Min Epoch: 31\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.8057  ... Validation Loss: 1.7461\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.3977  ... Validation Loss: 1.4293\n",
      "******.*..\n",
      "Epoch: 30/1000 ... Train Loss: 1.2729  ... Validation Loss: 1.3739\n",
      "*...... Early Stop; Min Epoch: 31\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.6374  ... Validation Loss: 0.5615\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 0.4727  ... Validation Loss: 0.4238\n",
      "****......\n",
      "Epoch: 30/1000 ... Train Loss: 0.3206  ... Validation Loss: 0.3708\n",
      " Early Stop; Min Epoch: 24\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.7276  ... Validation Loss: 1.6545\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.3024  ... Validation Loss: 1.2989\n",
      "***.*.*...\n",
      "Epoch: 30/1000 ... Train Loss: 1.1917  ... Validation Loss: 1.2758\n",
      "... Early Stop; Min Epoch: 27\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.4720  ... Validation Loss: 0.3929\n",
      "******.**.\n",
      "Epoch: 20/1000 ... Train Loss: 0.2871  ... Validation Loss: 0.2704\n",
      "***.*...*.\n",
      "Epoch: 30/1000 ... Train Loss: 0.2439  ... Validation Loss: 0.2632\n",
      "***..*....\n",
      "Epoch: 40/1000 ... Train Loss: 0.1871  ... Validation Loss: 0.2661\n",
      ".. Early Stop; Min Epoch: 36\n",
      "***.******\n",
      "Epoch: 10/1000 ... Train Loss: 1.9734  ... Validation Loss: 1.9025\n",
      "*****..***\n",
      "Epoch: 20/1000 ... Train Loss: 1.8313  ... Validation Loss: 1.7250\n",
      "****.**.**\n",
      "Epoch: 30/1000 ... Train Loss: 1.5758  ... Validation Loss: 1.4874\n",
      "***.*...*.\n",
      "Epoch: 40/1000 ... Train Loss: 1.5147  ... Validation Loss: 1.4306\n",
      "..... Early Stop; Min Epoch: 39\n",
      "***...**.*\n",
      "Epoch: 10/1000 ... Train Loss: 0.9482  ... Validation Loss: 0.9008\n",
      "******.*..\n",
      "Epoch: 20/1000 ... Train Loss: 0.7533  ... Validation Loss: 0.6981\n",
      "*....*....\n",
      "Epoch: 30/1000 ... Train Loss: 0.7488  ... Validation Loss: 0.6873\n",
      ".. Early Stop; Min Epoch: 26\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.8603  ... Validation Loss: 1.7680\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.5752  ... Validation Loss: 1.5536\n",
      "******.***\n",
      "Epoch: 30/1000 ... Train Loss: 1.4038  ... Validation Loss: 1.4324\n",
      ".***..**..\n",
      "Epoch: 40/1000 ... Train Loss: 1.3177  ... Validation Loss: 1.4022\n",
      ".*.*..**.*\n",
      "Epoch: 50/1000 ... Train Loss: 1.2863  ... Validation Loss: 1.3936\n",
      "*..*.*....\n",
      "Epoch: 60/1000 ... Train Loss: 1.2610  ... Validation Loss: 1.3929\n",
      ".. Early Stop; Min Epoch: 56\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.7061  ... Validation Loss: 0.6156\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 0.5810  ... Validation Loss: 0.5077\n",
      "*****.*...\n",
      "Epoch: 30/1000 ... Train Loss: 0.4454  ... Validation Loss: 0.4448\n",
      "*.*..**...\n",
      "Epoch: 40/1000 ... Train Loss: 0.3545  ... Validation Loss: 0.4027\n",
      ".*...... Early Stop; Min Epoch: 42\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.8752  ... Validation Loss: 1.7962\n",
      "*********.\n",
      "Epoch: 20/1000 ... Train Loss: 1.5011  ... Validation Loss: 1.5615\n",
      "******.**.\n",
      "Epoch: 30/1000 ... Train Loss: 1.3132  ... Validation Loss: 1.3918\n",
      ".*.****.*.\n",
      "Epoch: 40/1000 ... Train Loss: 1.2400  ... Validation Loss: 1.3747\n",
      ".**...... Early Stop; Min Epoch: 43\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.6902  ... Validation Loss: 0.6179\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 0.4997  ... Validation Loss: 0.4226\n",
      "***..*****\n",
      "Epoch: 30/1000 ... Train Loss: 0.3580  ... Validation Loss: 0.3524\n",
      "**.*......\n",
      "Epoch: 40/1000 ... Train Loss: 0.3141  ... Validation Loss: 0.3144\n",
      " Early Stop; Min Epoch: 34\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.9020  ... Validation Loss: 1.8183\n",
      "*********.\n",
      "Epoch: 20/1000 ... Train Loss: 1.7051  ... Validation Loss: 1.6323\n",
      "**..*.*.*.\n",
      "Epoch: 30/1000 ... Train Loss: 1.5948  ... Validation Loss: 1.5764\n",
      "*..*.*....\n",
      "Epoch: 40/1000 ... Train Loss: 1.4994  ... Validation Loss: 1.5216\n",
      "***.******\n",
      "Epoch: 50/1000 ... Train Loss: 1.4084  ... Validation Loss: 1.3746\n",
      "**.****.**\n",
      "Epoch: 60/1000 ... Train Loss: 1.3360  ... Validation Loss: 1.2882\n",
      "*...... Early Stop; Min Epoch: 61\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.7477  ... Validation Loss: 0.6586\n",
      "*.*.***.**\n",
      "Epoch: 20/1000 ... Train Loss: 0.6952  ... Validation Loss: 0.6156\n",
      "*.**.**..*\n",
      "Epoch: 30/1000 ... Train Loss: 0.5835  ... Validation Loss: 0.5508\n",
      "...... Early Stop; Min Epoch: 30\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.8160  ... Validation Loss: 1.7689\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.5348  ... Validation Loss: 1.5226\n",
      "****.*...*\n",
      "Epoch: 30/1000 ... Train Loss: 1.3271  ... Validation Loss: 1.3322\n",
      "..*..*.*.*\n",
      "Epoch: 40/1000 ... Train Loss: 1.2819  ... Validation Loss: 1.2946\n",
      "...*..*.*.\n",
      "Epoch: 50/1000 ... Train Loss: 1.2576  ... Validation Loss: 1.2754\n",
      "....*.....\n",
      "Epoch: 60/1000 ... Train Loss: 1.2430  ... Validation Loss: 1.2716\n",
      ". Early Stop; Min Epoch: 55\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.6364  ... Validation Loss: 0.5561\n",
      "*****...*.\n",
      "Epoch: 20/1000 ... Train Loss: 0.5487  ... Validation Loss: 0.5331\n",
      "*...******\n",
      "Epoch: 30/1000 ... Train Loss: 0.3770  ... Validation Loss: 0.3848\n",
      "*....**..*\n",
      "Epoch: 40/1000 ... Train Loss: 0.2871  ... Validation Loss: 0.3207\n",
      "*...... Early Stop; Min Epoch: 41\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.6731  ... Validation Loss: 1.5866\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.3325  ... Validation Loss: 1.3546\n",
      "*******.*.\n",
      "Epoch: 30/1000 ... Train Loss: 1.2463  ... Validation Loss: 1.2777\n",
      "***.**.***\n",
      "Epoch: 40/1000 ... Train Loss: 1.2080  ... Validation Loss: 1.2277\n",
      "...... Early Stop; Min Epoch: 40\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.4387  ... Validation Loss: 0.3646\n",
      "**...... Early Stop; Min Epoch: 12\n",
      "*****..***\n",
      "Epoch: 10/1000 ... Train Loss: 1.9697  ... Validation Loss: 1.9059\n",
      "*****..***\n",
      "Epoch: 20/1000 ... Train Loss: 1.6398  ... Validation Loss: 1.5988\n",
      "*.*******.\n",
      "Epoch: 30/1000 ... Train Loss: 1.4732  ... Validation Loss: 1.4414\n",
      "*****.....\n",
      "Epoch: 40/1000 ... Train Loss: 1.3999  ... Validation Loss: 1.3847\n",
      "*.**...*..\n",
      "Epoch: 50/1000 ... Train Loss: 1.3694  ... Validation Loss: 1.3718\n",
      "..*...*...\n",
      "Epoch: 60/1000 ... Train Loss: 1.3433  ... Validation Loss: 1.3462\n",
      ".*..**.*.*\n",
      "Epoch: 70/1000 ... Train Loss: 1.3211  ... Validation Loss: 1.3303\n",
      "*...*.**..\n",
      "Epoch: 80/1000 ... Train Loss: 1.3052  ... Validation Loss: 1.3207\n",
      "**.....*..\n",
      "Epoch: 90/1000 ... Train Loss: 1.2960  ... Validation Loss: 1.3213\n",
      ".... Early Stop; Min Epoch: 88\n",
      "****.*.***\n",
      "Epoch: 10/1000 ... Train Loss: 0.9318  ... Validation Loss: 0.8858\n",
      "****.***.*\n",
      "Epoch: 20/1000 ... Train Loss: 0.8343  ... Validation Loss: 0.7745\n",
      "******...*\n",
      "Epoch: 30/1000 ... Train Loss: 0.4446  ... Validation Loss: 0.3613\n",
      "..*...... Early Stop; Min Epoch: 33\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.8244  ... Validation Loss: 1.7256\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.4184  ... Validation Loss: 1.4114\n",
      "******..*.\n",
      "Epoch: 30/1000 ... Train Loss: 1.2378  ... Validation Loss: 1.2893\n",
      ".**....*..\n",
      "Epoch: 40/1000 ... Train Loss: 1.2052  ... Validation Loss: 1.2745\n",
      ".... Early Stop; Min Epoch: 38\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.6019  ... Validation Loss: 0.4860\n",
      "********.*\n",
      "Epoch: 20/1000 ... Train Loss: 0.4890  ... Validation Loss: 0.4122\n",
      ".*.******.\n",
      "Epoch: 30/1000 ... Train Loss: 0.3364  ... Validation Loss: 0.3296\n",
      "********.*\n",
      "Epoch: 40/1000 ... Train Loss: 0.2242  ... Validation Loss: 0.2726\n",
      "*.****....\n",
      "Epoch: 50/1000 ... Train Loss: 0.2105  ... Validation Loss: 0.2314\n",
      ".. Early Stop; Min Epoch: 46\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 1.8176  ... Validation Loss: 1.7215\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 1.3553  ... Validation Loss: 1.3684\n",
      "*********.\n",
      "Epoch: 30/1000 ... Train Loss: 1.2111  ... Validation Loss: 1.2878\n",
      ".**..**.**\n",
      "Epoch: 40/1000 ... Train Loss: 1.1702  ... Validation Loss: 1.2463\n",
      ".*..*.....\n",
      "Epoch: 50/1000 ... Train Loss: 1.1378  ... Validation Loss: 1.2281\n",
      ". Early Stop; Min Epoch: 45\n",
      "**********\n",
      "Epoch: 10/1000 ... Train Loss: 0.6324  ... Validation Loss: 0.4972\n",
      "**********\n",
      "Epoch: 20/1000 ... Train Loss: 0.4608  ... Validation Loss: 0.4178\n",
      "******.***\n",
      "Epoch: 30/1000 ... Train Loss: 0.2539  ... Validation Loss: 0.2833\n",
      "..***.*.**\n",
      "Epoch: 40/1000 ... Train Loss: 0.1767  ... Validation Loss: 0.2567\n",
      "**.*******\n",
      "Epoch: 50/1000 ... Train Loss: 0.1341  ... Validation Loss: 0.2138\n",
      "..*.*****.\n",
      "Epoch: 60/1000 ... Train Loss: 0.1052  ... Validation Loss: 0.2101\n",
      "*....*..*.\n",
      "Epoch: 70/1000 ... Train Loss: 0.0877  ... Validation Loss: 0.2054\n",
      "..*...... Early Stop; Min Epoch: 73\n"
     ]
    }
   ],
   "source": [
    "neural_offset = 5 # try 50-150 ms offset\n",
    "window_size = 70\n",
    "label_col = 'layout'\n",
    "func_dict = {'wiener': contrastive_functions.run_wiener, 'rnn': contrastive_functions.run_rnn}\n",
    "\n",
    "fpath = '../data/SPK20220308/neuron_num_results/'\n",
    "\n",
    "num_repeats = 10\n",
    "# num_neuron_list = np.arange(2,51,4)\n",
    "num_neuron_list = [2,4,6,8,10]\n",
    "\n",
    "\n",
    "num_neuron_results_dict = {'num_neuron_list': num_neuron_list}\n",
    "for repeat_idx in range(num_repeats):\n",
    "    rng = np.random.default_rng(repeat_idx) # new set of shuffled neurons seeded by repeat_idx\n",
    "    random_units = rng.choice(range(85), size=85).astype(str)\n",
    "\n",
    "    num_neuron_results_dict[f'repeat_{repeat_idx}'] = {'random_units': random_units}\n",
    "    for num_neurons in num_neuron_list:\n",
    "\n",
    "        # Filter neural_df with task info to random subset of neurons\n",
    "        task_unit_mask = np.in1d(task_neural_df['unit'].values, random_units[:num_neurons])\n",
    "        layout_mask = task_neural_df['unit'].str.contains(pat='layout')\n",
    "\n",
    "        task_neural_df_filtered = task_neural_df[np.logical_or.reduce([task_unit_mask, layout_mask])].reset_index(drop=True)\n",
    "\n",
    "        # Filter neural_df without task info to random subset of neurons\n",
    "        notask_unit_mask = np.in1d(notask_neural_df['unit'].values, random_units[:num_neurons])\n",
    "        notask_neural_df_filtered = notask_neural_df[np.logical_or.reduce([notask_unit_mask])].reset_index(drop=True)\n",
    "\n",
    "        df_dict = {'task': {'df': task_neural_df_filtered, 'task_info': True, 'num_cat': 4}, # num_cat = number of categorical features\n",
    "                   'notask': {'df': notask_neural_df_filtered, 'task_info': False, 'num_cat': 0}}\n",
    "        \n",
    "\n",
    "        decode_results = dict()\n",
    "        for func_name, func in func_dict.items():\n",
    "            decode_results[func_name] = dict()\n",
    "            for df_type, pred_df in df_dict.items():\n",
    "                model, res_dict = func(wrist_df, pred_df['df'], neural_offset, cv_dict, metadata, task_info=pred_df['task_info'],\n",
    "                                       window_size=window_size, num_cat=pred_df['num_cat'], label_col=label_col)\n",
    "\n",
    "                decode_results[func_name][df_type] = res_dict\n",
    "\n",
    "                # Save results on every loop in case early stop\n",
    "                num_neuron_results_dict[f'repeat_{repeat_idx}'][f'num_neuron_{num_neurons}'] = decode_results\n",
    "                #Save metadata\n",
    "                output = open(f'{fpath}num_neuron_results.pkl', 'wb')\n",
    "                pickle.dump(num_neuron_results_dict, output)\n",
    "                output.close()\n",
    "\n",
    "                if func_name == 'rnn':\n",
    "                    torch.save(model.state_dict(), f'{fpath}models/{df_type}_neurons{num_neurons}_repeat{repeat_idx}.pt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b48ab3240dc41ffa029ce879fa5e087e0a83cbe7a72ef65a19cb71535771faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
